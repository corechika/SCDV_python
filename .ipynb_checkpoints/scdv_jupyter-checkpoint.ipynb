{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import scipy as sp\n",
    "from collections import defaultdict\n",
    "from sklearn.mixture import GaussianMixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word vector のモデルファイルの読み込み\n",
    "def load_model(model_type='word2vec'):\n",
    "    if model_type == 'fasttext':\n",
    "        return gensim.models.KeyedVectors.load_word2vec_format('../fastText/build/model.vec', binary=False)\n",
    "    elif model_type == 'word2vec':\n",
    "        return gensim.models.KeyedVectors.load_word2vec_format('./entity_vector/wiki.model.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルを使って言葉のベクトルを取得 .binバージョン\n",
    "def get_word_vector(words, vec_size):\n",
    "    word_vec = defaultdict(list)\n",
    "    for word in words:\n",
    "        try:\n",
    "            word_vec[word].append(model[word])\n",
    "        except:\n",
    "            word_vec[word].append(np.zeros(vec_size, ))\n",
    "    return word_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 出現頻度の数え上げ(1単語は1データに1回)\n",
    "def count_freq(words):\n",
    "    frequency = defaultdict(int)\n",
    "    for i, word in enumerate(words):\n",
    "        word_list = []\n",
    "        for token in word.split(','):\n",
    "            if token in word_list:\n",
    "                continue\n",
    "            frequency[token] += 1\n",
    "            word_list.append(token)\n",
    "    return frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IDFの計算\n",
    "def calc_idf(n, word_freq):\n",
    "    return np.log2(n / word_freq) + 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GMM\n",
    "def training_GMM(wv, n_cluster, vec_size):\n",
    "    x = [vec[0].reshape(1, vec_size)[0] for word, vec in wv.items() if not np.all(vec[0] == 0)]\n",
    "    \n",
    "    gmm = GaussianMixture(n_components=n_cluster, covariance_type='tied')\n",
    "    gmm.fit(np.array(x))\n",
    "    return gmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "path = 'test.csv'\n",
    "scdv_column = 'hoge'\n",
    "words = pd.read_csv(path)[scdv_column].values\n",
    "\n",
    "# 単語頻度\n",
    "freq = count_freq(words)\n",
    "N = len(words)\n",
    "del words\n",
    "\n",
    "# modelファイルの読み込み\n",
    "model = load_model('fasttext')\n",
    "\n",
    "# modelのベクトルサイズ\n",
    "vec_size = model.vector_size\n",
    "print('vector length:{0}'.format(vec_size))\n",
    "\n",
    "# word vectorを取得\n",
    "wv = get_word_vector(freq.keys(), vec_size)\n",
    "\n",
    "# モデルの削除\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GMM\n",
    "cluster = 5\n",
    "gmm = training_GMM(wv, n_cluster=cluster, vec_size=vec_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCDV計算\n",
    "words = pd.read_csv(path)[scdv_column].values\n",
    "sentence_vec = []\n",
    "for row in words:\n",
    "    wtv = np.zeros(cluster*vec_size, )\n",
    "    for word in row.split(','):\n",
    "        if np.all(wv[word] == 0): continue\n",
    "        # idf\n",
    "        idf = calc_idf(N, freq[word])\n",
    "        # wcv_ik\n",
    "        wcv_ik = [prob * wv[word][0] for prob in gmm.predict_proba(wv[word])[0]]\n",
    "        # wtv_i\n",
    "        con = np.concatenate((wcv_ik[0], wcv_ik[1]))\n",
    "        if len(wcv_ik) > 2:\n",
    "            for wcv in wcv_ik[2:]:\n",
    "                con = np.concatenate((con, wcv))\n",
    "        wtv_i = con * idf\n",
    "        wtv += wtv_i\n",
    "    sentence_vec.append(wtv/len(row.split(',')))\n",
    "del words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scdv_df = pd.DataFrame()\n",
    "for i, s in enumerate(sentence_vec):\n",
    "    scdv_df = scdv_df.append(pd.Series(s, name='d_' + str(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scdv_df.to_csv(\"output/test_output.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
